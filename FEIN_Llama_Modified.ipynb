{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Llama on Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q transformers accelerate peft datasets\n",
    "!pip install -U bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import warnings\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, notebook_login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# Define BitsAndBytesConfig for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config, \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(r\"gbharti/finance-alpaca\", split=\"train\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    return f\"### Instruction:\\n{sample['instruction']}\\n\\n### Response:\\n{sample['output']}\"\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": format_prompt(x)}, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=64,  \n",
    "    lora_alpha=64, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,  \n",
    "    eval_dataset=None,  \n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,  \n",
    "        gradient_accumulation_steps=4,  \n",
    "        warmup_ratio=0.03,  \n",
    "        max_steps=4000,  \n",
    "        learning_rate=1e-4,  \n",
    "        logging_steps=10,  \n",
    "        output_dir=\"outputsLlama\", \n",
    "        optim=\"adamw_bnb_8bit\",  \n",
    "        save_strategy=\"epoch\",  \n",
    "        fp16=True,  \n",
    "        report_to=\"none\", \n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())  \n",
    "print(torch.cuda.device_count())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"Meta-Llama-3.1-8B-Finance-FineTune\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "peft_model = PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Code by: Sagar Thapliyal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load your custom FEIN dataset\n",
    "df = pd.read_csv(\"fein_dataset.csv\")  # Update path if needed\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format the dataset with FEIN-specific prompt style\n",
    "def format_prompt(sample):\n",
    "    return f\"\"\"You are FEIN, an AI Financial Assistant trained to give tailored financial advice.\n",
    "\n",
    "### User Query:\n",
    "{sample['instruction']}\n",
    "\n",
    "### FEIN's Advice:\n",
    "{sample['output']}\"\"\"\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"text\": format_prompt(x)}, remove_columns=dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04861970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the trainer with evaluation and logging\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.05,\n",
    "        max_steps=3000,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        output_dir=\"fein_outputs\",\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "        fp16=True,\n",
    "        report_to=\"none\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save and merge the LoRA model\n",
    "output_dir = \"fein-ai-llama3-8b-v1\"\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference example to test the model\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=output_dir, tokenizer=tokenizer)\n",
    "query = \"How should a 25-year-old plan for retirement if they earn â‚¹50,000 per month?\"\n",
    "response = pipe(query, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
